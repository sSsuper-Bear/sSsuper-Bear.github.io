<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>论文 | SuperBear's blog</title><meta name="robots" content="noindex"><meta name="description" content="论文摘要一般来说，互联网上的工作负载变化很快，但变化的规律还是有规律的。目前，工作负载预测已成为促进资源管理自动扩展的有前途的工具，从而降低成本并提高云中的资源利用率。当前大多数工作量预测方法都基于单一模型。然而，由于网络流量通常是混合的、不可分割的，单一的模型很难得到令人满意的预测性能。为了解决这个问题，本文提出了一种自适应的工作负荷预测方法。该方法首先将工作负载分类为不同的类别，根据工作负载特"><meta name="keywords" content="信号处理"><meta name="author" content="SuperBear"><meta name="copyright" content="SuperBear"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="https://sssuper-bear.github.io/2021/01/14/%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3/%E8%AE%BA%E6%96%87/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="论文"><meta property="og:url" content="https://sssuper-bear.github.io/2021/01/14/%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3/%E8%AE%BA%E6%96%87/"><meta property="og:site_name" content="SuperBear's blog"><meta property="og:description" content="论文摘要一般来说，互联网上的工作负载变化很快，但变化的规律还是有规律的。目前，工作负载预测已成为促进资源管理自动扩展的有前途的工具，从而降低成本并提高云中的资源利用率。当前大多数工作量预测方法都基于单一模型。然而，由于网络流量通常是混合的、不可分割的，单一的模型很难得到令人满意的预测性能。为了解决这个问题，本文提出了一种自适应的工作负荷预测方法。该方法首先将工作负载分类为不同的类别，根据工作负载特"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2021-01-14T13:45:41.000Z"><meta property="article:modified_time" content="2022-02-20T13:33:18.673Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.4.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: {"text":"bear,love,wind","fontSize":"15px"},
  medium_zoom: true,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2022-02-20 21:33:18'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@1,300&display=swap" rel="stylesheet"><link rel="stylesheet" href="\css\bilibiliBanner.css"  media="defer" onload="this.media='screen'"><link rel="stylesheet" href="\css\cursor.css"  media="defer" onload="this.media='screen'"><link rel="stylesheet" href="\css\myDesign.css"  media="defer" onload="this.media='screen'"><link rel="stylesheet" href="\css\custom.css"  media="defer" onload="this.media='all'"><style type="text/css">#toggle-sidebar {bottom: 80px}</style><script defer src="/live2d-widget/autoload.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="SuperBear's blog" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">56</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/videos/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg" data-type="color"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87"><span class="toc-number">1.</span> <span class="toc-text">论文</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-number">1.2.</span> <span class="toc-text">introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">文献综述</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#EMD"><span class="toc-number">1.3.1.</span> <span class="toc-text">EMD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Savitzky-Golay-Filter"><span class="toc-number">1.3.2.</span> <span class="toc-text">Savitzky-Golay Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Augmented-Dickey-Fuller-Test"><span class="toc-number">1.3.3.</span> <span class="toc-text">Augmented Dickey-Fuller Test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.3.4.</span> <span class="toc-text">注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TCN"><span class="toc-number">1.3.5.</span> <span class="toc-text">TCN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.6.</span> <span class="toc-text">混合模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#C-LSTM-Attention-model"><span class="toc-number">1.3.6.1.</span> <span class="toc-text">C-LSTM-Attention model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.3.6.2.</span> <span class="toc-text">混合模型的设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.3.7.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.3.8.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.3.8.1.</span> <span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IMF%E6%95%B0%E9%87%8F%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.3.8.2.</span> <span class="toc-text">IMF数量的选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-LSTM-Attention%E5%B1%82%E6%95%B0%E9%80%89%E6%8B%A9"><span class="toc-number">1.3.8.3.</span> <span class="toc-text">C-LSTM-Attention层数选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E6%95%B0%E6%8D%AE%E7%AA%97%E5%8F%A3%E5%A4%A7%E5%B0%8F%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.3.8.4.</span> <span class="toc-text">误差数据窗口大小的选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="toc-number">1.3.8.5.</span> <span class="toc-text">结果与讨论</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.4.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(/2021/01/14/%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3/%E8%AE%BA%E6%96%87/(%E8%AE%BE%E7%BD%AE%E9%A1%B6%E9%83%A8%E5%9B%BE))"><div id="winterBanner"><div class="view"><img class="morning" src="/bilibiliBanner/winter/bilibili-winter-view-1.png" alt=""><img class="afternoon" src="/bilibiliBanner/winter/bilibili-winter-view-2.png" alt=""><video class="evening" autoplay="" loop="" muted=""><source src="/bilibiliBanner/winter/bilibili-winter-view-3.webm" type="video/webm"></video><img class="window-cover" src="/bilibiliBanner/winter/bilibili-winter-view-3-snow.png" alt=""></div><div class="tree"><img class="morning" src="/bilibiliBanner/winter/bilibili-winter-tree-1.png" alt=""><img class="afternoon" src="/bilibiliBanner/winter/bilibili-winter-tree-2.png" alt=""><img class="evening" src="/bilibiliBanner/winter/bilibili-winter-tree-3.png" alt=""></div></div><script async data-pjax src="/js/bilibili-banner.js"></script><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">SuperBear's blog</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/videos/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">论文</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-01-14T13:45:41.000Z" title="发表于 2021-01-14 21:45:41">2021-01-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-02-20T13:33:18.673Z" title="更新于 2022-02-20 21:33:18">2022-02-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/study/">study</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>一般来说，互联网上的工作负载变化很快，但变化的规律还是有规律的。目前，工作负载预测已成为促进资源管理自动扩展的有前途的工具，从而降低成本并提高云中的资源利用率。当前大多数工作量预测方法都基于单一模型。然而，由于网络流量通常是混合的、不可分割的，单一的模型很难得到令人满意的预测性能。为了解决这个问题，本文提出了一种自适应的工作负荷预测方法。该方法首先将工作负载分类为不同的类别，根据工作负载特征自动分配给不同的预测模型。此外，通过建立混合0-1整数规划模型，将工作负载分类问题转化为任务分配问题，并提供在线解决方案。我们使用谷歌集群跟踪来评估所提出的方法。实验结果表明，与时间序列预测方法（自回归综合移动平均（ARIMA）、支持向量机（SVM）相比，该方法将平台累积相对预测误差分别提高了29.06%、8.42%和40.86% ) 和线性回归 (LR)。</p>
<p>近年来，云计算彻底改变了计算方式。尽管有很多好处，但它也面临一些挑战。云计算的主要挑战包括动态资源扩展和功耗。这些因素导致云系统变得低效且成本高昂。工作负载预测是可以提高云的效率和运营成本的变量之一。准确性是工作负载预测的关键组成部分，现有方法在产生 100% 准确结果方面滞后。研究人员也在不断努力改进它。在本文中，我们提出了一种使用神经网络和自适应差分进化算法的工作负载预测模型。该模型能够学习最佳的变异策略以及最佳的交叉率。实验是在 NASA 和 Saskatchewan 服务器的 HTTP 跟踪的基准数据集上针对不同的预测间隔进行的。我们将结果与基于众所周知的反向传播学习算法的预测模型进行了比较，并获得了显着的改进。所提出的模型在误差减少方面实现了高达 168 倍的偏移，并且预测误差减少了高达 0.001。</p>
<p>——越来越多的企业开始使用Docker容器搭建云平台。预测容器工作负载的资源使用情况一直是提高云计算平台性能的一个重要且具有挑战性的问题。现有的预测模型要么产生大量时间开销，要么精度不足。本文提出了 ARIMA 和三重指数平滑的混合模型。它可以准确预测容器资源加载序列中的线性和非线性关系。为了应对动态的 Docker 容器资源负载，混合模型中的两个单一模型的权重值是根据它们在一段时间内的预测误差的平方和来选择的。我们还设计并实现了一个实时预测系统，由Docker容器资源负载数据的收集、存储、预测以及基于预测值的CPU和内存资源使用率的调度优化组成。实验结果表明，与ARIMA、三重指数平滑模型和ANN+SaDE模型相比，混合模型的预测精度平均提高了52.64%、20.15%和203.72%，且时间开销较小。</p>
<p>凭借其快速的开发和部署，分布式云数据中心提供的大量云服务已成为互联网服务中最重要的组成部分。尽管有很多好处，但他们的提供商面临一些具有挑战性的问题，例如动态资源扩展和功耗。工作负载预测在解决这些问题中起着至关重要的作用。准确性和快速学习是关键性能。它一直在努力改进。本文提出了一种综合预测方法，将 Savitzky-Golay 滤波器和小波分解与随机配置网络相结合，以预测下一个时隙的工作量。在这种方法中，任务时间序列首先由 SG 滤波器平滑，然后通过小波分解将平滑的时间序列分解为多个分量。在此基础上，首次建立了一个综合模型，可以很好地刻画趋势和详细成分的统计特征。实验结果表明，它比一些有代表性的预测方法取得了更好的预测结果和更快的学习速度。</p>
<p>从业者须知——工作量预测很重要在构建可扩展的绿色分布式云数据中心方面发挥作用。本文提出了一种新颖且基本的方法来实现工作负载预测的准确性和快速学习。它开发了一种集成预测方法，将 Savitzky-Golay 滤波器和小波分解与随机配置网络相结合，以预测下一个时隙的工作量。为了对获得的信息建立精细的预测模型，同时获得更好的预测结果和更快的学习速度，本文提出了一种集成方法SGW-S，用于构建任务时间序列的预测模型并确定其最优模型参数。这</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">近年来，云计算彻底改变了计算方式，一般来说，互联网上的工作负载变化很快，但变化的规律还是有规律的。目前，工作负载预测已成为促进资源管理自动扩展的有前途的工具，从而降低成本并提高云中的资源利用率。云计算的主要挑战包括动态资源扩展和功耗。这些因素导致云系统变得低效且成本高昂。工作负载预测是可以提高云的效率和运营成本的变量之一。准确性是工作负载预测的关键组成部分。当前大多数工作量预测方法都基于单一模型。然而，由于网络流量通常是混合的、不可分割的，单一的模型很难得到令人满意的预测性能。在本文中，我们提出了一种集成经验模式分解（EMD）、长短期记忆（LSTM）和注意力机制的短期负荷预测方法。首先，用 EMD 将负载序列分解为几个固有模式函数 (IMF)，将每个提取的IMF作为一个特征输入，然后将不同的基于注意力机制的LSTM神经网络应用于整合的特征集。最后通过动态权重调整得到最终的负载预测结果。实验是在 NASA 和 Saskatchewan 服务器的 HTTP 跟踪的基准数据集上进行的。实验结果表明，与ARIMA、三重指数平滑模型和LSTM模型相比，混合模型的预测精度平均提高了52.64%、20.15%和203.72%。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于特征提取表示效率低下并且缺乏丰富的上下文依赖性，视频预测具有挑战性。提出了一种使用具有双注意力神经网络 (DA-DMLSTM) 的双记忆 LSTM 进行视频预测的新方法。提出了一种新的具有较少门结构的循环单元，称为 DMLSTM。它用于通过适当地利用差分操作来提取视频序列的表示。为了充分利用历史表征，双重注意力机制被设计为分别从时间和空间维度捕捉长期的时空交互。然后，将双重注意力嵌入到 DMLSTM 中，构建 DA-DMLSTM 神经网络，使模型对短期动态和长期上下文表示具有更大的建模能力。实验结果表明，通过与一些最新技术的比较，所开发的方法具有出色的性能。</span><br><span class="line">关键词：视频预测；双记忆LSTM；双重关注；历史表现</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">云计算的负载预测具有挑战性，</span><br><span class="line">设计了一种集成经验模式分解（EMD）、长短期记忆（LSTM）和注意力机制的负载预测方法。提出</span><br><span class="line">单一的模型很难得到令人满意的预测性能。通过混合模型，并设计了一种自适应权重变换方法。</span><br><span class="line">深层特征难以挖掘，提取表示效率低下</span><br><span class="line">实验结果表明，通过与一些最新技术的比较，所开发的方法具有出色的性能。</span><br><span class="line"></span><br><span class="line">关键词：</span><br></pre></td></tr></table></figure>



<h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>随着云计算平台的发展和普及，大多数企业都有自己的数据中心。通过向用户提供各种虚拟资源，如计算资源、存储资源和网络资源，用户可以以相对较低的成本获得高质量、强安全性和高度可扩展的基础设施服务[1]。然而，随着云计算平台的不断扩展，虚拟机[2]存在运行效率低、启动慢等问题。为了缓解这些问题，Docker容器[3]作为一种新的虚拟化技术应运而生。无论是在虚拟机还是Docker容器中，我们都应该为应用程序分配足够的资源以使其顺利运行。但是，在大多数情况下，应用程序并不是以最重的负载运行的，预配置的资源在大多数情况下都是空闲的。这造成了资源的浪费。此外，当工作量繁重且必须竞争时。如果同时使用其他应用程序，预分配的资源可能不够。为了解决这些问题，云计算中通常使用特定的预测算法来预测资源需求，并提前进行资源分配优化，以提高资源利用率和服务质量。</p>
<p>目前，关于Docker集装箱资源负荷预测的相关研究较少。Shanmugam等人[4]通过ARIMA模型预测了容器的CPU使用情况，然后使用基于循环的算法将负载分配给容器的web服务。Roy等人[5,6]提出了一种基于ARIMA模型的云计算负载预测模型，该模型首先对时间序列进行平滑处理。Huang等人[7]提出了一种基于二次指数平滑的资源预测模型，用于预测客户需要订阅的云资源。它不仅考虑了当前资源状况，还考虑了历史资源记录，获得了较高的预测精度。</p>
<p>上述几种类型的资源负荷预测基于ARIMA模型或二次指数平滑模型。这是因为资源负载序列是一个时间序列，这两个模型是时间序列预测的常用预测模型。但无论是ARIMA模型还是二次指数平滑模型，本质上都是线性模型。但是，Docker容器中不同资源负载生成的时间序列不仅是线性的，而且是非线性的，如图1所示。这两个模型在预测集装箱资源装载序列中的非线性关系时没有很好的预测精度。然而，尽管ARIMA可以在容器和虚拟机中使用，但由于资源使用顺序不稳定。</p>
<p>可以看出，现有的模型要么无法预测线性和非线性工作负载，要么不适合在容器环境中使用。针对上述问题，本文提出了一种ARIMA和三指数平滑模型的混合模型来预测Docker集装箱资源负载中的线性和非线性关系。为了处理动态Docker集装箱资源负载，混合模型中的两个模型的权重值根据其各自在一段时间内预测误差的平方和来选择。在预测过程中，利用ARIMA模型挖掘集装箱资源装载序列的线性关系，消除集装箱资源装载序列的随机波动；利用三指数平滑法挖掘非线性关系，平滑集装箱资源装载序列。我们还设计并实现了一个Docker集装箱工作量实时预测系统。该系统实现了Docker容器资源负载数据的收集、存储、预测以及基于预测值的CPU和内存资源使用的调度优化。</p>
<p>本文的贡献如下：</p>
<p>•我们提出了一种ARIMA和三指数平滑的混合预测模型，可以预测集装箱装载序列中的线性和非线性关系，</p>
<p>并显著提高Docker集装箱装载预测精度我们设计了一个Docker容器负载预测系统，该系统能够预测多维资源负载，</p>
<p>并根据预测值自动对CPU和内存资源使用进行调度优化我们在模拟和真实云环境中对各种Docker容器上的混合模型进行了评估。实验结果表明，该混合模型具有良好的预测精度和较小的时间开销</p>
<p>云计算是一项很有前途的技术，旨在基于按使用付费模式将各种可视化资源、软件和平台作为服务提供给客户[1]。为了向最终用户提供高性能的云服务，在云DCs中进行资源管理非常重要[2,3]，它可以降低能耗成本和二氧化碳排放[4,5]。一般来说，资源管理方案可分为被动和主动两类，在第一种情况下，当工作量增加/减少到预定义的特定阈值时，将进行资源管理[6]。但是，关于虚拟机的启动时间，反应式方法无法处理突然爆发的工作负载[7] 并且可能会导致服务级别协议（SLA）vio-lations。另一方面，主动预防性方法通过重新认识可能的资源使用模式并提供所需资源来预测DC的未来工作负载，从而解决了这一问题。因此，通过有效的预测，可以阻止性能下降，减少空闲资源，进一步提高利润。然而，进行主动资源管理并不是一个简单的过程，云托管服务的可变工作负载可能会导致以下问题。</p>
<p>资源调配不足：应用程序无法获得足够的资源来处理其所有请求，并可能导致SLAV</p>
<p>过度资源调配：虚拟资源分配给应用程序的数量超出了需要，这会给客户带来更多成本。然而，在某种程度上，需要过度调配来处理某种程度上的工作负载波动</p>
<p>振荡：由于自动扩展，会出现过度配置和不足配置问题的组合。</p>
<p>因此，准确的工作负载预测是实施有效的主动式资源管理方案和为用户请求分配按需资源的关键因素[8]。因此，云资源管理系统一方面应该能够分配所需的虚拟资源，以防止性能损失，另一方面应该通过取消分配空闲资源（自动缩放）来防止资源浪费[9,10]。为了解决这些问题，如图1所示，应该监控云托管服务，并记录它们的负载。然后，可以处理这些历史负载数据，并将其输入到工作负载预测器中，以预测未来的负载。为此，可以在预测过程中使用各种资源，例如CPU、内存、网络带宽，甚至I/O操作。使用这些信息，资源管理和自动缩放方案可以根据需要放大/缩小虚拟资源[11]。通常，云资源可以水平和垂直扩展，在水平情况下[12]，将提供更多虚拟机，以应对未来负载，在垂直情况下，应增加现有虚拟机的资源[7、13、14]，但由于存在安全风险，操作系统通常不允许进行此类更改[15–18]。</p>
<p>然而，云计算中的工作负载预测是一个具有挑战性的问题，因为与HPC系统和网格计算不同，云工作负载具有更高的方差、更短、更具交互性，其平均噪声几乎是网格计算的20倍。此外，由于云资源由多个用户或任务共享，它们可能会受到一些波动的影响，并且新的工作负载模式也会不断出现。此外，云基础设施中的非平稳工作负载（其模式随时间变化）使得预测模型的重新训练更加频繁，并相应地增加了开销。为了解决这些问题，并且考虑到准确的工作负载预测在云DCs的有效资源管理中的重要性，使用FIG进行负载预测受到了极大的关注。1使用工作量预测的弹性不同。各种数学模型和基于机器学习的预测算法[8,19–24]。本文对最先进的工作量预测方案、其应用技术以及实施这些方案的动机进行了全面的调查。它根据应用的预测方法对这些方案进行分类，并描述每个框架如何尝试预测未来负载，并将这些结果用于资源管理、自动缩放和调度。在对文献进行深入分析之后，本文提供了开放性的研究问题，为以后的研究奠定基础。</p>
<p>云计算（CC）是一种将资源和应用程序带给用户的新技术。任务调度在CC[1]中是一个必不可少的问题。云服务提供商（CSP）和云用户（CU）之间的关联由SLA管理。此外，SLA被认为是CC环境中最重要的先决条件，因此CU服务的最大可用性可以说是以最少的资源分配给每个CU，以最小化服务器基础结构成本。因此，SLA定义了服务水平及其相关成本。服务水平协议（SLA）是ser的一个要素CSP和CU之间正式描述服务的副合同。服务的具体组成部分包括范围、定义质量方面以及CSP和CU之间接受的相应责任。SLA包括某些参数，每个服务元素的最低质量水平在CSP和CU之间安排。SLA被认为是每个元素中最主要的组成部分，因为云用户提供商的目标仍然存在。</p>
<p>为每个CU分配较少的资源需求，以降低其服务器基础架构成本。为确保SLA反复聚合，这些协议定期制定，具有特定的差异化线，相关用户需要经常会面，以确保顺利沟通。同时，避免因未能提供约定服务而受到处罚。未能向CU提供所需服务称为SLA冲突。此外，顾客服务提供商要求避免因未能提供分配的服务而受到处罚。提供服务的疏忽被称为违反SLA。相反，CU更喜欢按需获得服务，并且不受任何干扰。尽管这些高可用率，违规行为据说发生在真实世界场景中，并导致CSP和CU的巨额成本。因此，与基于客户的协议相比，服务水平协议在云环境下得到了扩展，并由CSP作为基于服务的协议提供。</p>
<p>许多研究工作都是为了在云中分配任务和平衡负载而设计的。但是，如果同时减少SLA违反和能耗仍然是一个悬而未决的问题，则TSE并不有效。此外，现有工程中违反SLA的情况并未减少。因此，需要一种新的技术来有效地调度用户任务，并在云服务提供期间获得较少的SLA冲突。因此，本研究设计了NMT-FOLS技术。[2]中设计了一种概率方法，旨在提高云中负载平衡任务调度的性能。然而，最大完工时间更高。[3]中提出了一种两阶段策略，以提高任务调度性能并减少虚拟机在云中分配任务的等待时间。但是，谢霆锋的得分更低。</p>
<p>为了在云中获得最佳的资源利用率，在[4]中开发了资源感知负载平衡算法（RALBA）。但是，它未能在云中获得有效的负载平衡。基于生物地理学的优化（BBO）技术是在[5]中设计的，以增强VM的布局。但是，没有考虑能源消耗。</p>
<p>[6]中提出了一种高效的负载平衡感知云资源调度算法，以提高移动用户的资源效率和体验质量。这里，资源调度据说是通过平衡负载来完成的。此外，还对用户服务质量进行了测量。然而，SLA违反仍然是一个悬而未决的问题。为了解决SLA违规问题，在[7]中提出了一个自回归综合移动平均（ARIMA）模型，用于查找云中的工作负载。在这里，违规行为是根据移动平均法来衡量的，移动平均法可以根据不同情况而有所不同CS分配给CU的工作负载和可用工作负载。</p>
<p>[8]中解决了服务器整合问题，目的是降低数据中心的功耗。在此，分析了整合过程中的功耗。但是，没有考虑CPU时间、内存和带宽利用率。[9]中采用了时间序列预测来预测工作负载和资源供应。然而，调度性能很差。</p>
<p>为了在大量性能指标之间实现良好的平衡，在[10]中设计了一个新的VM整合框架。在这里，为每个VM分配了工作负载，这样每个VM在分配工作负载时都会照顾其用户。但是，内存利用率和CPU时间非常高。在[11]中，引入了基于模糊的多维资源调度模型，以减少内存消耗和CPU时间。上述两个目标是根据资源调度策略达成的。尽管内存利用率被认为是最优的，但CPU时间和带宽使用率却更高。</p>
<p>任何方法中涉及的违规数量都是衡量SLA违规的主要标准。据说，vio-lations越小，该方法的效率就越高。在[12]中，提出了基于贝叶斯和集群的负载平衡（LB-BC），以减少失败任务部署的数量。但是，日程安排所需的时间没有得到解决。为了解决这个问题，在[13]中，设计了一个动态任务分配策略，其核心目标是在给定的约束条件下，使执行过程中涉及的完工时间和成本最小化，并对其进行了分析。但是，计划的任务数的比率不够。</p>
<p>[14]中执行了增强的负载平衡元启发式调度，以平衡整个云系统的负载。在这种元启发式调度模型中，负载被称为以这样一种方式平衡，即CU发出的请求由CSP执行。但是，平衡负载所需的计算时间仍然是一个悬而未决的问题。为了最大限度地减少负载平衡所需的计算时间，开发了基于混合方法的资源和负载平衡框架[15]。混合方法旨在通过具有统一负载分布的虚拟机优化资源并实现负载平衡。但是，没有考虑在CS上完成任务所使用的能量量。[16]中设计了一种混合元启发式算法。然而，最大完工时间更大。</p>
<p>NMT-FOLS技术旨在解决上述现有问题。NMT-FOLS技术的主要贡献解释如下。</p>
<ul>
<li>为了提高任务调度的效率，从而减少云服务呈现过程中与尖端作品相比对SLA的侵犯，提出了NMT-FOLS技术。与相对传统的工作相反，NMT-FOLS技术设计用于多目标萤火虫优化算法中彩票调度的应用。</li>
<li>为了在现有工作的同时最大限度地减少云中的资源浪费和能源使用，在NMT-FOLS技术中引入了MFO-TS和MFOLS算法。通过使用MFO-TS和MFOLS算法流程，NMT-FOLS技术找到了分配任务和向云用户提供所需服务的最佳虚拟机。</li>
</ul>
<p>纸张的剩余结构如下所示。第2节介绍了相关工作。第3节通过架构图说明NMT-FOLS技术。拟议NMT-FOLS技术的实验设置如第节所示。4.第5节展示了实验结果和对比结果分析。第6节最后描述了本文的结论</p>
<p>云计算（也称为按需计算）在过去几年中得到了显著的扩展。公司正在通过云启动应用程序并加快运行速度。它使用户能够在第三方数据中心存储和处理数据。通过互联网提供服务的虚拟计算资源的使用得到了工业界和学术界的极大关注。在传统计算中，用户可以访问固定数量的计算资源。另一方面，云计算中的ondemand资源正在向其客户提供[1]。所以，云对于避免公司的前期基础设施成本非常有帮助。</p>
<p>由于云的健壮性、可扩展性、按需服务等特性，许多组织正在转向云。服务提供商部署大型数据中心，以便为客户提供按需服务。这些数据中心是云计算环境的骨干。云数据中心背后的关键概念是虚拟化，它有助于通过虚拟机（VM）在多个用户之间共享资源。为了在保持服务质量（QoS）的同时最大化服务提供商的收益，数据中心需要一种高效、动态的资源扩展和分配策略。</p>
<p>由于在托管应用程序生命周期内的任何时间点添加或删除虚拟硬件资源的灵活性，动态扩展以及高级资源管理的可能性增加了[2]。动态资源扩展已成为数据中心以完美的方式工作的关键关注点。资源扩展取决于几个因素，包括活动用户的数量、即将发生的事件、当前由于数据中心的预测负载将决定可扩展的资源量，因此需要建立一个有效可靠的预测系统，以准确估计即将到来的负载。此信息还可用于提高资源利用率和功耗。因此，云数据中心可以以较低的成本运行，同时减少违反SLA的情况。精确预测的关键挑战是和不同数量的客户机的交互以及工作负载的高度非线性。可以通过几种方法预测工作量。可以测量指定时间间隔内的最大或平均工作负载。然而，基于统计的方法，如平均值和最大值是非常普遍的，不能产生准确的预测。例如，如果我们使用基于最大工作负载的预测模型，那么资源将在大多数时间保持未使用状态。另一方面，如果使用平均方法来预测未来的工作量，那么系统将见证资源的缺乏。当工作负载增加时，这将导致性能下降。基于这些方法的预测模型被认为是较差的，因为它们在预测方面不好，并且对应的案例数量很少[3]。机器学习方法被广泛用于建立更准确的预测模型。机器学习技术使用历史数据作为训练窗口，在整个预测间隔内预测工作量[4]。其中预测间隔定义了每次预测之间的时间。</p>
<p>本文致力于开发基于神经网络和自适应差分进化的工作负荷预测模型，该模型能够以更高的精度预测工作负荷。该模型不使用均值等简单的统计信息，而是从工作负载中学习和提取模式。获得的模式用于进一步的预测。该模型采用进化方法训练，以最小化初始解选择的影响。进化算法使用一组解在多个方向上探索空间。进化算法的难点之一是参数调整。由于所提出的预测模型能够学习交叉率、变异率和变异策略，因此这种影响被最小化。这些预测可进一步用于改进资源扩展决策。该模型在两台服务器的基准数据集上进行了测试，并与反向传播神经网络模型进行了比较。所提出的模型输出器形成了基于平均、最大和反向传播网络的预测模型，大大降低了均方预测误差。</p>
<p>论文的其余部分组织如下：第2节提供了相关工作的概述。第3节讨论了建议的方法，第4节讨论了结果和讨论。最后，本文在第5节中总结了结论性意见和未来的范围。</p>
<h3 id="文献综述"><a href="#文献综述" class="headerlink" title="文献综述"></a>文献综述</h3><p><strong>为何</strong></p>
<p>云计算是一种将资源和应用程序带给用户的新技术，由于云的健壮性、可扩展性、按需服务等特性，许多组织正在转向云。通过向用户提供各种虚拟资源，如计算资源、存储资源和网络资源，用户可以以相对较低的成本获得高质量、强安全性和高度可扩展的基础设施服务[1]。我们应该为应用程序分配足够的资源以使其顺利运行。但是，在大多数情况下，应用程序并不是以最高的负载运行的，预配置的资源在大多数情况下都是空闲的，这会造成了资源的浪费。此外，当工作量繁重且必须竞争时。如果同时使用其他应用程序，预分配的资源可能不够。为了在保持服务质量（QoS）的同时最大化服务供应商的收益，数据中心需要一种高效、动态的资源扩展和分配策略。为了解决这些问题，云计算中通常使用特定的预测算法来预测资源需求，并提前进行资源分配优化，以提高资源利用率和服务质量。资源需求量的预测是影响弹性管理算法性能好坏的关键，需求量预测过高会导致额外的资源分配，造成资源浪费; 需求量预测不足，用户的部分资源请求得不到满足，则会降低服务质量。因此，提高云平台资源需求量的预测精度，是一个亟待解决的问题。</p>
<p>然而，自动缩放 Web 应用程序仍然具有挑战性，主要是由于动态且难以预测工作负载强度和特征。即使对于单个应用程序，不同的用户也经常有不同的使用模式和周期性</p>
<p><strong>现状</strong></p>
<p>在过去的几十年里，很多方法被提出进行负载预测，这些方法主要分为三类：</p>
<p>第一类是传统时间序列预测方法，包括差分自回归移动平均法(Auto Regressive ‍Integrated‍ Moving‍ Average，ARIMA)[9]、‍线性回归、指数平滑法[10]等。Calheiros等人[9]他们采用基于自回归集成移动平均（ARIMA）模型的预测器来实现对虚拟机（VM）实例的预测扩展。它们展示了ARIMA执行短期预测的能力，以及使用这些预测的主动扩展对应用程序观察到的QoS的影响。Tan等人[12]提出了一种结合ARIMA、移动平均（MA）、指数平滑（ES）和神经网络的实现对交通流预测聚合方法。使用ARIMA、MA和ES模型获得相关的时间序列，将其作为神经网络的输入。上述几种类型的资源负荷预测基于ARIMA模型或二次指数平滑模型。这是因为资源负载序列是一个时间序列，这两个模型是时间序列预测的常用预测模型。但无论是ARIMA模型还是二次指数平滑模型，本质上都是线性模型。但是，不同资源负载生成的时间序列不仅是线性的，而且是非线性的，使用传统时间序列的预测方法对于非线性情况预测效果并不理想。</p>
<p>第二类是机器学习算法，包括支持向量机(Support‍ Vector‍ Machine，SVM)、人工神经网络(Artificial‍ Neural ‍Network，ANN)以及循环神经网络(Recurrent‍ Neural ‍Network，‍RNN)等。例如Liu等人[17]先基于负载数据的特点进行负载分类，然后使用SVM预测波动‍较大的负载类型，通过实验证明了其方法的优越性。文献[14]分别比较了‍ANN、RNN、ARIMA和SVM四种模型在Google集群数据集上的预测效果，结果发现RNN取得了最高的预测精度。文献[15]率先将长短时记忆网络(Long‍ Short Term ‍Me‍mory，LSTM)应用到了CPU使用率的预测中，结果发现LSTM与传统时间序列预测方法‍ARIMA相比预测效果更好，然而这些方法只针对了单一负载，且准确度不高。文献[16]设计了双向的LSTM来对容器云平台中部署的容器负载进行预测，相比于LSTM准确性有所提高，但是仍然没有解决只能预测单一负载问题。近年来，文献[17]研究发现，相比于RNN和LSTM等循环神经网络及‍其变体，时间卷积网络(Temporal‍ Convolutional ‍Network，TCN)在多个时间序列预测任务上表现的更好，且具有更稳定的梯度，更小的训练占用内存，更灵活的感受野以及能够并行运算的效率优势，然而机器学习算法只针对了单一负载，且准确度不高。</p>
<p>第三类是组合预测模型，通过将不同的模型组合起来进行预测，充分利用不同模型之间的优势。文献[18]通过结合二次指数平滑法与灰色预测模型对容器的CPU使用‍率进行预测，将负载类型区分为正常负载与异常负载，分别使用不同的方法进行预测，并基于预测结果在负载高峰期对应用进行提前扩容，但该预测方法对于波动较大的负载预测效果较差。文献[19]研究提出了基于ARIMA和三次指数平滑法的容器负载组合预测方法，综合考虑了容器资源负载数据中所具有的线性关系和非线性关系，组合两种预测方法对两种关系分别进行捕捉，有效提高了预测精度，但是对于不同的负载泛化能力较差。文献[20]将三次指数平滑法与TCN进行了加权组合，通过实验对包括CPU使用率、内存使用量等云资源负载进行了预测。但其使用了固定权重，权重信息不能自适应变换，泛化能力较差。文献[21]提出ARIMA-RBF‍组合预测模型，首先使用‍ARIMA‍对时序数据进行建模，捕捉其中的线性关系，然后使用‍RBF[22]‍模型对‍ARIMA‍产生的预测误差进行建模，得到误差预测值，最后将‍ARIMA‍和‍RBF‍模型的预测结果相加，得到最终的预测值。实验结果表明，ARIMA-RBF‍模型相比于‍ARIMA‍和‍RBF‍模型，在预测精度上有更好的效果，但其只能对短期负载预测，不能预测长期负载。‍文献[23]在Savitzky-Golay滤波器/小波分解[24]、[25]和随机配置网络[26]的基础上，建立了Distributed cloud data centers (DCDCs)工作负荷预测的集成模型SGW-‍S，并在基准数据集上进行测试，并与其他方法进行比较，该方法在准确性和学习速度等方面均优于其他方法，但是小波分解，分解的波形较少，特征还不明确，预测结果还能进一步提升。文献［27］在1998 年提出的一种时频分析的自适应方法EMD。EMD不仅突破了傅里叶变换的局限性，而且在确定小波基函数时避免了小波变换的主要问题，还具有良好的时频分辨率和适应性，能够完美地重建原始信号，同时具有突出信号中可能忽略的精细结构的潜力。同属于时序预测的风速预测，已经使用神经网络与EMD算法进行了结合。文献[28]提出了一种基于反向传播神经网络(BPNN)和经验模态分解(EMD)组合的方法被用来进行短期风速预测，每个独立方法的优势被合并到一个组合模型中，以一种独立的方式改进每个模型的结果。对于中期预测，最近的研究多采用混合方法，如EMD - ANNS[29,30]，因为方法组合优于单一方法。最近，EMD结合特征选择和人工神经网络的混合方法被提出[31,32]，得到了比经典分解预测模型更好的结果。这些研究表明，这两种方法的优势可以利用在一个联合的方法。在此基础上，对EMD - ANN算法进行了改进和改进。文献[33]在此基础上，对EMD - ANN算法进行了改进。提出了一种基于自适应噪声(CEEMDAN)完整集成经验模态分解(complete ensemble empirical mode decomposition with adaptive noise，简称CEEMDAN)模型的集成，并对单基模型进行了显著改进。文献[34]在也采用CEEMDAN分解，并结合经验小波变换(EWT)对得到的第一个子序列进行进一步分解，得到的子序列的复杂度最高。一些小波算法如小波包分解(wavelet packet decomposition, WPD)和小波包滤波(wavelet packet filter, WPF)作为分解算法也在其他研究中得到了成功的应用[35,36]，但目前还没在云负载中进行试验。目前的组合预测模型虽然取得了较好的预测结果，但还是存在特征提取不充分，预测结果不够高的问题。</p>
<p><strong>总结，针对他们存在的问题 ， 提出我的方法</strong></p>
<p>针对上面提到的一些问题，比如特征提取不足，非线性序列难以预测。本文利用EMD算法能够更好地处理非线性负载，并且提取序列表层特征，再利用ADF算法对分解出来的序列进行筛选，筛选出稳定可预测的分量。再使用两套不同深度的LSTM对分解后的负载进行特征提取，将提取出的特征使用注意力机制对不同的特征给予不同的权重，再将其整合输出。模型A，注意力集中在最近5帧，对于短期负载预测效果更好，模型B，注意力位于整体序列，对于整体预测效果更好。我们提出一种自适应权重变化的机制，对模型A、B分于不同的权重，根据其预测结果误差的变化，对其权重的额分配进行一个改变，以达到一个较好的预测效果。</p>
<p>The rest of this paper is organized as follows. 第二节介绍了一些算法，以及我们提出模型的细节。第三节，我们通过各种评估指标比较了所提出的模型与其他主流方法的预测性能，并提供了一个全面的实验分析。第四部分阐述了结论并分析了未来的发展情况。</p>
<p>然而，已有的预测方法常根据单个序列蕴含的信息建立预测模型，尽管集成方法考虑不同模式的 负载序列，但也仅是多个预测模型的简单结合，并未利用负载间的关系． 而当负载序列大量增加时，序列间相关性的影响作用会增大，故仅依赖单个负载 序列信息构建预测模型，难免无法对影响负载变化 趋势的因素进行多视角考虑和利用． 另一方面，从 构建预测模型的角度看，预测性能的好坏与数据中 蕴含的有效信息量具有直接关系，有效信息量越多， 预测性能越好． 而负载序列之间的相关程度对有效 信息的提取和获得将产生重要影响．本文致力于开发基于神经网络和自适应差分进化的工作负荷预测模型，该模型能够以更高的精度预测工作负荷。</p>
<p>基于以上分析可知，若能合理利用相关负载序列之间的关系信息，则可弥补单个负载序列建模信 息不足的缺陷，有效提高负载预测模型的准确性． 因此，笔者提出一种基于时序相关性的云平台多负 载序列联合预测方法，其新颖性在于通过挖掘负载 序列在时序特征空间存在的真实相关性，并利用该 相关关系实现相关负载序列的联合预测，从而提高 云平台负载预测精度． 具体地说，该方法首先利用 深度学习模型长短时记忆( LSTM，long short-term memory) 网络［10-11］提取负载序列的时序特征，将原始空间的序列特征表示变换到序列的时序特征空 间，消除原始序列中部分的噪声和冗余信息，实现对 序列内在信息的刻画; 然后利用层次聚类方法对提 取的序列时序特征聚类，从而获得特征空间上相似 的负载序列; 最后利用多任务学习( MTL，multi-task learning) ［12］能捕捉学习任务间相关信息的特点，对获得的相似负载序列类建立 MTL 模型，挖掘和利用 相似负载序列间的共享领域知识，补充单个负载序 列信息，从而实现多个负载序列的联合预测</p>
<p>The rest of this paper is organized as follows. 第二节介绍了我们所用到技术的基本法方法，以及模型的细节。第三部分，介绍了C-LSTM-Attention方法，以及。第四节，我们通过各种评估指标确定了所提方法的最佳条件。比较了所提出的模型与其他主流模型的预测性能，并提供了一个全面的实验分析。第五部分阐述了结论并分析了未来的发展情况。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="EMD"><a href="#EMD" class="headerlink" title="EMD"></a>EMD</h3><p>经验模态分解(Empirical Mode Decomposition, EMD)在处理非平稳及非线性数据上具有明显的优势，适合分析非线性非平稳的信号序列。原始数据被分解为有限个本征信号（Intrinsic Mode Function, IMF）和一个残差信号，分解出来的各个<code>IMF</code>分量包含了原信号的不同时间尺度的局部特征信息。一个<strong>本征模函数必须满足以下两个条件：</strong></p>
<ul>
<li>函数在整个时间范围内，局部极值点和过零点的数目必须相等或最多相差一个</li>
<li>在任意时刻点，局部最大值的包络（上包络线）和局部最小值的包络（下包络线）平均必须为零，即上下包络线相对于时间轴局部对称</li>
</ul>
<p>EMD算法的过程包含以下几步：</p>
<ol>
<li><p>对一个给定的时间序列x(t)，标出局部极值点，以及最大最小值点</p>
</li>
<li><p>通过三次样条插值（cubic spline line）连接极大值点构成上包络线（upper envelope）e<sub>min,1</sub>,连接极小值点构成下包络线（lower envelope）e<sub>max,1</sub>，求上下包络线的均值e<sub>mean</sub>。<br>$$<br>e _ { mean,1 } ( t ) = \frac { [ e _{min,1} ( t ) + e _ { max,1} ( t ) ] } { 2 }<br>$$</p>
</li>
<li><p>然后用输入信号减去上下包络线的均值<br>$$<br>h _ { 1 } = x _ { ( t ) } - e _ { mean, 1}<br>$$</p>
</li>
<li><p>将h1(k-1)作为新的x(t)，重复筛选步骤(步骤1-3)k次。那就是<br>$$<br>h_{1(k)}=h_{1(k-1)}-e_{mean,1(k)}<br>$$</p>
</li>
<li><p>重复筛选过程，直到h1(k)满足IMF的要求，该信号作为一个IMF分量c<sub>1</sub>。通过将c1与其他数据分离，得到残差r1.<br>$$<br>r_{1} = x_{(t)} - c_{1}<br>$$</p>
</li>
<li><p>将残渣r1视为新x(t)，重复上述筛选过程(步骤1-5)。最后原始时序序列被分解为：<br>$$<br>x ( t ) = \sum _ { i = 1 } ^ { n } (c_i ) + r _ { n }<br>$$<br>端点扩展问题一直是EMD算法中一个重要的问题。在通常情况下，端点不是局部极值，因此需要扩展两个极大极值和两个最小极值来得到极值的包络。在[33]中，利用特征波得到最大极值和最小极值。然而，不同的特征波会导致不同的结果，很难在每次迭代中选择合适的波。本文采用了由Dawid Laszuk实现的EMD的python <strong>PyEMD</strong>包对原始信号进行分解。根据[51]，只要镜像靠近边缘的极值(或镜像扩展法)，就可以得到较好的结果。在一些文献中提出了一些改进的方法来减轻EMD的端点效应。例如在[52]中，高频使用端镜扩展，低频使用最小二乘多项式扩展。然而，目前还没有完整的解决方案，这意味着EMD方法端点效应的解决方案还有改进的空间</p>
</li>
</ol>
<h3 id="Savitzky-Golay-Filter"><a href="#Savitzky-Golay-Filter" class="headerlink" title="Savitzky-Golay Filter"></a>Savitzky-Golay Filter</h3><p>为了对非平稳任务时间序列进行进一步平滑，以去除异常值和噪声。SG滤波器[13]是一种称为最小二乘多项式平滑的数据平滑方法。它可以消除噪声，保持信号的峰值和宽度。在工作负载采集或传输过程中，一些数据可能会出现异常或丢失，这可能会导致数据干扰。</p>
<p>一个时间序列为$$X = { x _ { 1 } , x _ { 2 } , \cdots , x _ { t } }$$,取原始数据中xi处左右各M个样本点，并将xi设为原点，即构造一个以xi为中心包含2M+1个采样点的窗口数组，而后构造一个p阶多项式来拟合该数组：</p>
<p>$$<br>q ( n ) = \sum _ { k = 0 } ^ { p } a _ { k } n ^ { k },n\in [ - m , m ],p\leq2M+1<br>$$</p>
<p>当式(9)取最小值时拟合效果最好，而后通过移动该窗口数组即可得到原始数据的所有拟合点。在拟合过程中，过于偏离正常趋势曲线的噪声部分会被丢弃，因而该方法能够对数据起到平滑滤波的作用。<br>$$<br>C = \sum _ { n = - m } ^ { m } ( q ( n ) - x (n) ) ^ { 2 }<br>$$</p>
<h3 id="Augmented-Dickey-Fuller-Test"><a href="#Augmented-Dickey-Fuller-Test" class="headerlink" title="Augmented Dickey-Fuller Test"></a>Augmented Dickey-Fuller Test</h3><p>采用augmented Dickey–Fuller (ADF) 测试序列是否平稳。他可以根据序列中是否存在单位根来检验高阶回归过程的平稳性。如果不存在单位根，则序列是平稳的；否则，序列不平稳。</p>
<p>ADF检验的无效假设是时间序列是非平稳的。如果处理后的IMF分量通过平稳性检验，则使用该分量作为输入，否则删除不平稳分量。在本文中，ADF水平分别设置为1%、5%和10%。它们代表了对原始假设的否定程度。其中，1%的临界值严格拒绝了原始假设。如果ADF值小于临界值1%，则序列是平稳的。此外，在本文中，大p阶表示非平稳性，小p阶表示平稳性。如果显著性水平0.05或0.01为阈值，则如果p阶大于所选显著性水平0.05或0.01，则需要差异；否则，无效假设将被拒绝。我们对Google工作负载数据集应用ADF单位根测试。ADF测试结果如表I所示。</p>
<p>为了确定任务时间序列是否平稳，需要将检测到的ADF值与临界值进行比较。我们可以观察到ADF值小于临界值1%，p阶小于显著水平0.01。也就是说，数据是固定的。此外，我们还测试了白噪声误差，其结果为零。p阶远小于显著水平0.01，且数据不是白噪声。这些结果表明，该序列不是完全随机的，是一个可预测的序列。因此，我们可以得出结论，预处理的任务时间序列大部分是平稳的，而不是白噪声。因此，可以使用我们提出的工作量预测模型对其进行分析</p>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>注意力机制是充分利用有限的资源从大量信息中快速选择几个关键信息，而忽略其余不重要的信息。其目的是从复杂信息中快速提取有效特征，筛选目标信息，通过调整注意力权重加强有效信息，过滤或弱化冗余信息。</p>
<p>假设LSTM的输入向量为x1，x2，···，xt，而对应LSTM的隐藏状态分别为h1, h2, ···, ht 。注意力机制的核心思想是计算某个时间步对应的隐藏状态 hi 与与最终的隐藏状态h‘的相似度S<sub>i</sub>。$$S _ { i } = \tanh ( W _ { n } \cdot h _ { i } + b _ { n } )$$</p>
<p>其中 Wn 和 bn 是网络的权重矩阵和偏置向量。每个时间步输入信息的注意力权重αi为</p>
<p>$$\alpha _ { i } = \frac { e x p ( S _ { i } * h^{‘} ) } { \sum _ { j = 1 } ^ { t } e xp (  S _ { j } * h^{‘} ) }$$</p>
<p>$$\alpha _ { i } = \frac { e x p ( S _ { i }*h’) } { \sum _ { j = 1 } ^ { t } e x p(S_{ j } * h’ ) }$$</p>
<p>其中 h<sup>‘</sup>是随机初始化的超参数，其中是通过训练学习获得的</p>
<p>注意力机制的输出为：</p>
<p>$$c _ { t  }= \sum _ { j = 1 } ^ { T } \alpha _ { tj }h_{j}$$</p>
<h3 id="TCN"><a href="#TCN" class="headerlink" title="TCN"></a>TCN</h3><p>TCN是一种融合扩张因果卷积（dilated causal convolution， DCC） 和 残 差 连 接 （residual connections，RC）的，可用于时间序列预测的神经网络模型［20］，网络架构如图所示。</p>
<p>如图1所示，左侧为 TCN，由多个 TCN残差块堆叠而成。其中，每个TCN残差块有一个重要参数对( k，d)，分别表示卷积核大小和扩张系数。右侧为TCN残差块的内部结构图，可以看出， TCN 残差块的最终输出为两条路径输出之和，其 中，一条路径为：输入经过两层相同的DCC并输出。 首先，输入进入第 1层权重初始化后的DCC；然后， 将其输出经过ReLU激活函数进行非线性变换；最 后，将非线性输出进行Dropout正则化，以降低模型 的过拟合，并输入到第 2 层 DCC 再次进行同样变 换。另一条路径为：输入经过一维卷积层直接到达 输出端。该路径为RC，源于残差神经网络，可以缓 解深层神经网络存在的梯度消失和梯度爆炸问题， 有助于构建深层神经网络［21］。</p>
<p>TCN的核心组件为DCC。DCC在因果卷积的基础上，增大了扩张系数d的值，从而扩大了网络的感受野，即可以接受更长的历史数据。首先，是因果卷积的应用，表示从未来到过去没有信息泄露［20］，</p>
<p>如图2左侧部分所示，其为3层因果卷积网络,示意图，该网络中卷积核为 2，扩张系数为 1，感受野为3（图中黑色带箭头虚线部分）。图中以带有箭头 的虚线表示卷积运算。由图 2 可知，预测负荷序 列 ŷ t由输入序列[ xt- 2，xt- 1，xt ]计算得出，而与输入序列[ xt+ 1，xt+ 2，⋯ ]无关［22］。由此可知，在TCN中应用因果卷积后不会造成信息泄露。其次，因果卷积存在感受野小的问题。所以，通过增大扩张系数来扩大网络感受野，从而构成了 DCC。DCC图2右侧部分所示。由图 2可知， DCC在相同层数下的感受野扩大到 4。所以，TCN 应用DCC后，可以接收更长历史序列数据。扩张卷 积运算如下式所示[20].</p>
<p>$$F ( t ) = \sum _ { v = 0 } ^ { u - 1 } f ( v ) X _ { t - d v }$$</p>
<p>式中：F ( t )表示扩张卷积运算；Xt- dv 表示序列数据；<br>f ( v )表示过滤器函数；u为输入序列数据长度；v为 输入序列数据中第v个元素值。</p>
<h3 id="混合模型"><a href="#混合模型" class="headerlink" title="混合模型"></a>混合模型</h3><h4 id="C-LSTM-Attention-model"><a href="#C-LSTM-Attention-model" class="headerlink" title="C-LSTM-Attention model"></a>C-LSTM-Attention model</h4><p>通过引入注意力机制，建立了如图4所示的C-LSTM-Attention神经网络，由输入层、特征提取层、注意力层和输出层组成。在输入层，对原始信号进行分割，构造满足要求的输入样本。首先将输入进行卷积，因为卷积可以有效的提取出时间序列维度间短期的相关性。对特征提取层包括几个不同的部分，包括单层LSTM、双层LSTM、三层LSTM等。不同层数的LSTM提取的特征是不同的，LSTM层数越多，提取的特征越深。通过调整模型中LSTM的深度，可以提取原始信号中从浅到深的多个不同尺度的特征。注意力层缝合不同深度的特征形成特征矩阵，优化权重参数，输出到输出层。输出层由两个全连接层组成；第一层主要用于将注意力模块的输出扁平化为一维序列，并通过线性校正单元（ReLU）层，提高整个模型的非线性表达能力。第二层是全连接层，生成预测输出值。</p>
<h4 id="混合模型的设计"><a href="#混合模型的设计" class="headerlink" title="混合模型的设计"></a>混合模型的设计</h4><p>如何确定A和B组合的权重系数是混合模型设计的关键。常用的方法有等权重平均系数法和基于误差指标的权重系数确定法。等加权平均系数法赋予A和B相同的权值。由于这两种模型的预测精度不同，特别是在容器资源负载动态变化的情况下。也就是说，它可能发生在某一时间段内，A模型表现较好，且B模型在下一时间段表现较好;或A在一个应用程序工作负载上预测得更好，而B在另一个应用程序上预测得更好。为了更好地适应负载的动态变化，我们选择了算法1中基于误差指标的权重系数确定方法。</p>
<p>由于在连续预测的过程中错误也会不断地产生,为了使权重系数跟上变化的时间序列容器资源使用情况,预测模型中的权重可以通过预测误差的平方和来动态设置。假设时间序列窗口的大小为size，我们对最新的预测误差进行积累，当预测误差的个数达到size的个数，则对误差进行比较，误差大的权重占比小，误差小的权重占比大。这样，两个单一模型的权重可以随着容器资源使用时间序列的变化而调整。由于初始情况没有误差信息，我们使用的初始权值为0.5，随着误差的不断出现，以及权值的不断更新，初始权值的设置影响可以忽略。</p>
<p>在算法 1 中，Err_A  和 Err_B 是滑动的用数组实现的windows，分别记录了A模型和B模型在前期的预测误差。 Sum_A和 Sum_B 分别是两个模型的预测误差平方和。混合模型算法的执行过程可以分为以下几个步骤，如图2所示。</p>
<ol>
<li>每次更新负载使用数据时，都会更新容器资源使用的时间序列。同时，计算 A 模型和B模型的预测误差，以更新混合模型的权重系数。</li>
<li>更新的时间序列分别使用A模型和B模型进行预测。</li>
<li>根据A模型和B模型的预测结果和预先确定的权重系数，混合模型的预测结果是A模型的预测结果乘以相应的权重系数和B模型的预测结果乘以相应的权重系数。</li>
<li>得到下一个时间点容器资源使用数据的真实值后，回到步骤1进行下一次预测</li>
</ol>
<p>该算法如算法 2 所示。 Pred_A和 Pred_B分别表示 A模型和B模型在一个周期内的预测值。 Err_A和 Err_B表示A模型和B模型的预测误差。 Pred_At+1 和 Pred Bt+1 分别表示 A模型和B模型在下一周期的预测值。Weight_A 和Weight_B 分别表示 A模型和B模型的权重。</p>
<p>假设工作负载序列的长度容器资源为 L。在 ARIMA 模型中，假设自回归模型的阶数为 p，移动平均模型的阶数为 q，其中 p 和 q 的取值范围为 0 ≤ p，q ≤ N，并且 p 和 q 不能都为零。 ARIMA模型算法的主要计算成本是确定自回归模型的阶数和移动平均模型的阶数以及计算自相关系数和偏自相关系数。它的时间复杂度为 O(Nq ln L+L(p+q)+qp2N2)。三重指数平滑算法的运行时间为 O(1)。</p>
<p>误差窗口的确定：实验1-8个窗口，看RMSE如何</p>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>In this paper, two error measures are used to examine the accuracy of a prediction model: root mean square error (RMSE) and Mean Absolute Percentage Error (MAPE). They are defined as：</p>
<ol>
<li>for,end for,do,while, end while,if,else都用加粗</li>
<li>变量要用花体</li>
<li>不要使用括号，而是使用缩进表示代码块</li>
<li>需要有input，output</li>
<li>需要有行标</li>
</ol>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li>不同数据集的时间间隔(1,5,10,20,30,50)</li>
<li>window_size(1,2,3,4,5)</li>
<li>数据集划分6:4</li>
<li>epoch200</li>
<li>loockback5</li>
</ul>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><ul>
<li><p>arima</p>
</li>
<li><p>lstm</p>
</li>
<li><p>TCN</p>
</li>
<li><p>attention</p>
</li>
<li><p>arima+SG</p>
</li>
<li><p>LSTM+SG</p>
</li>
<li><p>LSTM+Attention</p>
<ul>
<li>1LSTM+Attention</li>
<li>2LSTM+Attention</li>
<li>3LSTM+Attention</li>
</ul>
</li>
<li><p>EMD+LSTM</p>
</li>
</ul>
<h4 id="IMF数量的选择"><a href="#IMF数量的选择" class="headerlink" title="IMF数量的选择"></a>IMF数量的选择</h4><p>EMD 可以将原始负载序列分解为多个 IMF。为了了解不同 IMF 对负荷预测准确性的影响，进行了一系列实验。（该实验，使用的是C-LSTM-Attention model作为预测，TCN模型的IMF实验结果与C-LSTM-Attention model相似）。如表 2 所示，实验结果列于表 2 中，其中 N - 1 负载预测的准确性随着 IMF 数量的增加而首先出现。当 IMF 的数量为 3 或 4 时，准确度达到最高值。然后，当 IMF 的数量从 4 变为 6 时，预测准确度降低。所有三个指标（MAE、RMSE 和 MAPE）都有相似的实验结果。此外，随着 IMF 数量的增加，训练模型所花费的时间也会增加。因此，在使用本文提出的方法时，推荐的 IMF 数量为 3 或 4。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NASA					RMSE		MAE			MAPE</span><br><span class="line">1IMF_C_3LSTM_Attention	29.14		8.33		3.79</span><br><span class="line">2IMF_C_3LSTM_Attention	25.97		5.96		3.24</span><br><span class="line">3IMF_C_3LSTM_Attention	16.84		5.35		2.94</span><br><span class="line">4IMF_C_3LSTM_Attention	20.57		5.93		3.49</span><br><span class="line">5IMF_C_3LSTM_Attention	22.05		6.32		3.52</span><br><span class="line">6IMF_C_3LSTM_Attention	24.02		7.72		4.74</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Saskatchewan				RMSE		MAE		MAPE</span><br><span class="line">1IMF_C_3LSTM_Attention		11.63		7.73	16.74</span><br><span class="line">2IMF_C_3LSTM_Attention		5.89		3.09	7.06</span><br><span class="line">3IMF_C_3LSTM_Attention		3.40		1.68	4.06</span><br><span class="line">4IMF_C_3LSTM_Attention		3.37		1.53	3.54</span><br><span class="line">5IMF_C_3LSTM_Attention		3.76		2.00	4.25</span><br><span class="line">6IMF_C_3LSTM_Attention  	5.85		3.30	7.10</span><br></pre></td></tr></table></figure>



<h4 id="C-LSTM-Attention层数选择"><a href="#C-LSTM-Attention层数选择" class="headerlink" title="C-LSTM-Attention层数选择"></a>C-LSTM-Attention层数选择</h4><p>对于输入注意力层的特征，通过LSTM层提取特征，聚合多个LSTM网络提取的特征，再输入注意力网络。层数越多所得到的特征越深入，为了了解不同的LSTM特征提取层数对于结果的影响，进行了一系列的实验对LSMT特征提取层数进行验证。实验结果列于表3中，如表3所示，其中负载的准确性，随着LSTM层数增加而增加，然而当LSTM层数到达4层后，预测准确度开始下降，此外，随着LSMT层数的增加，训练模型所花的时间也在增加。因此，对于LSTM_Attention模型层数，推荐使用三层LSTM。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NASA						RMSE		MAE			MAPE</span><br><span class="line">3IMF_C_1LSTM_Attention		(26.91)		6.01		2.85</span><br><span class="line">3IMF_C_2LSTM_Attention		20.57		5.93		3.49</span><br><span class="line">3IMF_C_3LSTM_Attention		16.84		5.36		2.94</span><br><span class="line">3IMF_C_4LSTM_Attention		21.01		5.30		2.66</span><br><span class="line">3IMF_C_5LSTM_Attention		25.97		5.96		3.24</span><br><span class="line">3IMF_C_6LSTM_Attention		(26.64)		6.11		3.32</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Saskatchewan				RMSE		MAE		MAPE</span><br><span class="line">3IMF_C_1LSTM_Attention		5.71		3.77	7.95</span><br><span class="line">3IMF_C_2LSTM_Attention		4.19		2.08	4.63</span><br><span class="line">3IMF_C_3LSTM_Attention		3.40		1.68	4.06</span><br><span class="line">3IMF_C_4LSTM_Attention		3.62		1.64	3.71</span><br><span class="line">3IMF_C_5LSTM_Attention		3.57		1.56	5.59</span><br><span class="line">3IM_CF_6LSTM_Attention		(4.21)		2.42	4.93</span><br></pre></td></tr></table></figure>

<h4 id="误差数据窗口大小的选择"><a href="#误差数据窗口大小的选择" class="headerlink" title="误差数据窗口大小的选择"></a>误差数据窗口大小的选择</h4><p>对于不同大小的size，分析过多的误差数据可能会导致权重更新期间误差较大无法却及时纠正，造成误差累积，但是，如果使用的误差数据太小，可能出现只是一小段A模型比B模型好的情况。因此选择合适大小的误差数据对于预测系统至关重要。以下实验使用不同大小的误差数据得到的预测结果。结果如图所示。从图可以看出，当窗口大小在1-2之间，随着窗口大小的增加，将使用更多的误差数据，从而提高预测的准确性。然而，随着窗口的进一步增加，预测精度开始下降。此外，随着使用更多的历史数据，预测时间必会增加。综上所述，A model、B model混合模型的窗口大小为5.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NASA			RMSE	MAE		MAPE</span><br><span class="line">window_size=1	6.52	3.43	2.46	</span><br><span class="line">window_size=2 	10.68	5.20	3.21</span><br><span class="line">window_size=3 	13.33	5.58	3.39</span><br><span class="line">window_size=4 	10.63	5.39	3.29	</span><br><span class="line">window_size=5 	12.59	5.63	3.38</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Saskatchewan	RMSE	MAE 	MAPE</span><br><span class="line">window_size=1	2.35	1.35	3.03</span><br><span class="line">window_size=2	3.12	1.80	4.23</span><br><span class="line">window_size=3	3.42	1.85	4.41</span><br><span class="line">window_size=4	3.58	1.88	4.47</span><br><span class="line">window_size=5	3.51	1.87	4.52</span><br></pre></td></tr></table></figure>



<h4 id="结果与讨论"><a href="#结果与讨论" class="headerlink" title="结果与讨论"></a>结果与讨论</h4><p>为了进一步定量评估所提出方法的性能，选择了包括 ARIMA、Holt-Winters、TCN、LSTM 、在内的一些方法在NASA HTTP trace和Saskatchewan HTTP traces数据集分别在相同条件下进行测试。部分比较结果分别见表2和表3</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NASA					RMSE		MAE			MAPE</span><br><span class="line"></span><br><span class="line">ARIMA					43.75</span><br><span class="line">triple exponential smoothing</span><br><span class="line">holt—winter				(41.39)</span><br><span class="line">LSTM					31.29</span><br><span class="line">TCN						(30.68)</span><br><span class="line">EMD-TCN					14.82		</span><br><span class="line">EMD-c-LSTM-Attention	16.84		5.36		2.94</span><br><span class="line">ours					6.52		3.43		2.46</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Saskatchewan			RMSE		MAP		MAPE</span><br><span class="line">                          </span><br><span class="line">ARIMA					(15.67)</span><br><span class="line">holt—winter				(14.34)</span><br><span class="line">LSTM					14.32</span><br><span class="line">TCN						14.09</span><br><span class="line">EMD-TCN					5.31</span><br><span class="line">EMD-c-LSTM-Attention	3.40		1.68	4.06</span><br><span class="line">ours					2.35		1.35	3.03</span><br></pre></td></tr></table></figure>

<p>随着IMFs的增加，它所花费的时间也在增加</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>对负载进行预测，自动缩放应用程序，使计算中心以最大限度地提高性能成本是云计算中一个活跃的研究领域。本文提出了EMD_Attention动态神经网络用于负载预测。由于负载序列比较复杂，神经网络直接特征提取不准确，提出了通过结合EMD_ADF获取负载特征，并利用多个模型的输出动态调整预测结果，这样的架构设计使模型具有更强的鲁棒性。实验结果表明，通过与一些研究的方法进行比较，我们的方法具有优异的性能。</p>
<p>负载预处理及特征提取<br>$$<br>MAE = \frac { 1 } { n } \sum _ { i = 0 } ^ { n } | y _ { i } - \hat{ y_i } |<br>$$<br>NASA HTTP有NASA两个月的HTTP请求<br>$$<br>\hat{ y_i }<br>$$</p>
<!-- flag of hidden posts --></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">SuperBear</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sssuper-bear.github.io/2021/01/14/%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3/%E8%AE%BA%E6%96%87/">https://sssuper-bear.github.io/2021/01/14/%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3/%E8%AE%BA%E6%96%87/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sssuper-bear.github.io" target="_blank">SuperBear's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/">信号处理</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/12/26/论文相关/ADF检验/" title="ADF检验"><img class="relatedPosts_cover" src="https://img2.huashi6.com/images/resource/thumbnail/2021/11/24/153858_53852353058.jpg?imageMogr2/quality/75/interlace/1/thumbnail/700x%3E/format/webp"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-26</div><div class="relatedPosts_title">ADF检验</div></div></a></div><div class="relatedPosts_item"><a href="/2021/12/24/论文相关/Savitzky-Golay 滤波器/" title="Savitzky-Golay 滤波器"><img class="relatedPosts_cover" src="https://img2.huashi6.com/images/resource/thumbnail/2021/12/20/104347_22742393889.jpg?imageMogr2/quality/100/interlace/1/thumbnail/2000x%3E"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-24</div><div class="relatedPosts_title">Savitzky-Golay 滤波器</div></div></a></div><div class="relatedPosts_item"><a href="/2021/12/26/论文相关/差分与复原/" title="差分与复原"><img class="relatedPosts_cover" src="https://img2.huashi6.com/images/resource/thumbnail/2021/11/24/1766_76656829365.jpg?imageMogr2/quality/75/interlace/1/thumbnail/700x%3E/format/webp"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-26</div><div class="relatedPosts_title">差分与复原</div></div></a></div><div class="relatedPosts_item"><a href="/2021/12/24/论文相关/经验模态分解（EMD）/" title="经验模态分解"><img class="relatedPosts_cover" src="https://img2.huashi6.com/images/resource/p21525/2021/10/15/173852_73251356583.jpg?imageMogr2/quality/75/interlace/1/thumbnail/700x%3E/format/webp"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-24</div><div class="relatedPosts_title">经验模态分解</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By SuperBear</div><div class="footer_custom_text"><p><a style="margin-inline:5px" target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a><a style="margin-inline:5px" target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a><a style="margin-inline:5px" target="_blank" href="https://github.com/sSsuper-Bear"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a></p></div><div id="workboard"></div><script async="async" src="/js/runtime.js"></script></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script>function loadValine () {
  function initValine () {
    window.valine = new Valine({
      el: '#vcomment',
      appId: 'Fg783sRWVmsp38mbk5aoRuUa-gzGzoHsz',
      appKey: '6gcVwkjI0xDpUrjttJYRV8BH',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      master: 'E87E352DFDC19E444DAB2914B4952849'.split(','),
      friends: 'B923A10A9B7378EF516A1E15A6C77034'.split(','),
      tagMeta: '博主,小伙伴,访客'.split(',')
    });
    if ('nick,mail') { valine.config.requiredFields= 'nick,mail'.split(',') }
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script async src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"></script><script async src="//at.alicdn.com/t/font_2264842_3izu8i5eoc2.js"></script><div class="aplayer no-destroy" data-id="000PeZCQ1i4XVs" data-server="tencent" data-type="artist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><script defer src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/hexo-theme-volantis@latest/source/js/issues.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/third-party/click_heart.js" async="async"></script><script src="/js/third-party/ClickShowText.js" async="async"></script><link rel="stylesheet" href="/"><script src="/"></script><script src="/"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  'meta[name=description]',
  '#config_change',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

const pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
})

document.addEventListener('pjax:complete', function () {
  refreshFn()

  $('script[data-pjax]').each(function () {
    $(this).parent().append($(this).remove())
  })

  GLOBAL_CONFIG.islazyload && lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  if (typeof gtag === 'function') {
    gtag('config', '', {'page_path': window.location.pathname});
  }

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

})

document.addEventListener('pjax:send', function () {
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  $(window).off('scroll')

  //reset readmode
  $('body').hasClass('read-mode') && $('body').removeClass('read-mode')

  //reset font-size
  $('body').css('font-size') !== originFontSize && $('body').css('font-size', parseFloat(originFontSize))
})</script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_injector_config();
  }
  </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax src="https://unpkg.zhimg.com/hexo-butterfly-clock/lib/clock.min.js"></script><!-- hexo injector body_end end --></body></html>